name: test_job1
schedule_interval: '0 6 * * *'
engine: default_spark

depends_on: test_job2
airflow_dag_args:
  start_date: 20190430
  catch_up: true
  retries: 1

sources:
  - type: query
    sql: >-
      SELECT id, title, actor, target
      FROM events
      WHERE pt_date="{{ ds }}"
    alias: daily_events

computes:
  - type: sql
    sql: >-
      SELECT actor.id, count(*) as daily_amount
      FROM daily_events
      GROUP BY actor.id

sinks:
  - type: redis
    host: 127.0.0.1
    port: 6380