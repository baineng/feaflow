name: feast_job1
engine: default_spark

scheduler:
  schedule_interval: '0 6 * * *'
  start_date: 2021-09-10 00:00

sources:
  - type: pandas
    dict:
      id: [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010]
      title: ["message.send", "user.login", "user.signup", "user.logout", "message.send", "user.login", "user.signup", "user.logout", "user.signup", "user.logout"]
      published: ["2021-09-10", "2021-09-11", "2021-09-12", "2021-09-13", "2021-09-14", "2021-09-15", "2021-09-16", "2021-09-17", "2021-09-18", "2021-09-19"]

  - type: pandas
    file:
      type: csv
      path: "{{ project_root }}/../data/pandas_df1.csv"
      args:
        index_col: 0

computes:
  - type: sql
    sql: |-
      SELECT id,
             SUM(CASE WHEN title="message.send" THEN 1 ELSE 0) send_message_amount,
             SUM(CASE WHEN title="user.signup" THEN 1 ELSE 0) signup_times,
             SUM(CASE WHEN title="user.login" THEN 1 ELSE 0) login_times,
             SUM(CASE WHEN title="user.logout" THEN 1 ELSE 0) logout_times,
             "{{ ts }}" timestamp
      FROM (
        SELECT * FROM {{ source_0 }}
        UNION ALL
        SELECT * FROM {{ source_1 }}
      ) t
      GROUP BY title

sinks:
  - type: table
    name: test_sink_table
    mode: append
    format: orc

feature_view:
  name: fview_1
  entities:
    - name: id as user_id
      type: int
      desc: user's id
      labels: [fview_1_id]
  features:
    - name: send_message_amount
      type: int
    - name: signup_times
      type: int
    - login_times